{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Replicating / trying out Kozlowski data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "import smart_open\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname, start_year, end_year, limit=None):\n",
    "        self.dirname = dirname\n",
    "        self.start_year = start_year\n",
    "        self.end_year = end_year\n",
    "        self.limit = limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        # iterate through each the compressed file directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # for each compressed file open it\n",
    "            with gensim.utils.smart_open(os.path.join(self.dirname, fname)) as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    if len(line)<3:\n",
    "                        continue\n",
    "                    ngram = line[0]\n",
    "                    try:\n",
    "                        year = int(line[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    match_count = int(line[2])\n",
    "                    if year < self.start_year or year > self.end_year:\n",
    "                        continue\n",
    "                    # lower case the ngram, remove pos\n",
    "                    processed_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()]\n",
    "                    for x in range(match_count):\n",
    "                        yield processed_ngram\n",
    "\n",
    "class MySentsSimple:\n",
    "    def __init__(self, fpath, limit=None):\n",
    "        self.fpath = fpath\n",
    "        self.limit = limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        # for each compressed file open it\n",
    "        with smart_open.open(self.fpath, encoding=\"utf-8\") as infile:\n",
    "            for line in itertools.islice(infile, self.limit):\n",
    "                    line = gensim.utils.to_unicode(line).split(\"\\t\")\n",
    "                    if len(line)<3:\n",
    "                        continue\n",
    "                    ngram = line[0]\n",
    "                    try:\n",
    "                        year = int(line[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    match_count = int(line[2])\n",
    "                    if year < self.start_year or year > self.end_year:\n",
    "                        continue\n",
    "                    # lower case the ngram, remove pos\n",
    "                    processed_ngram = [word.split(\"_\")[0] for word in ngram.lower().split()]\n",
    "                    for x in range(match_count):\n",
    "                        yield processed_ngram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "##Reads ngrams one-by-one into word2vec##\n",
    "zip_fpath = \"../data/google_news_embedding.zip\"\n",
    "sentences = MySentsSimple(zip_fpath, limit=20) # a memory-friendly iterator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK\u0003\u0004-\u0000\u0000\u0000\t\u0000^mOG\u0015-Fÿÿÿÿÿÿÿÿ\u0018\u0000\u0014\u0000GoogleNews_Embedding.csv\u0001\u0000\u0010\u0000üñ7C\u0001\u0000\u0000\u0000vN6E\u0000\u0000\u0000\u0000tÿK,Í$ìíÏ¯(9Ã%k\u0018y­*\u0011\n",
      "EÈ\u0015¿5okÌ\fF\u0000\u0012\n",
      "\u0000è¯¥ÿ=Ã=ÂÜ\u001EMTõâê¬¬\u000F¿©©ªýþýüýÿ¾ÿÜÆîã?ñçøÏÇøÏçøÏ×øÏeÿ½ÌÿÎ_ºÌßºÌ_»Ìß»Ì_¼Ìß¼Ì_½Ìß½Îß½Îß½Îß½Îß½Îß½Îß½Îß½Îß½Îß½Îß½Íß½Íß½\u001D\u001Fvþîmþîmþîmþîmþîmþîmþî}þî}þî}þîýø¦ówïówïówïówïówïów\u001Fów\u001Fów\u001Fów\u001Fów\u001FÇe¿û¿û¿û¿û¿û¿û¿û¿û¿û¿û<®ñüÝçüÝçüÝçüÝù»\u001Fów?æï~Ìßý¿û1÷ã¸Aów?æï~Ìßý¿û9÷sþîçüÝÏù»ów?çï~\u001EwwþîçüÝ¯ù»_ów¿æï~Íßý¿û5÷kþî×üÝ¯chcã\u0018\u001CÛ1:¶cxlÇøØ\u0001²\u001D#d;Èví\u0018$Ûñ.5Äw9\u0007Ù9ÊÎav³s #í\u001CjÇX»\u001Cír=Gêñ.Çx»\u001C\u0003îr¸Ë1ä.Ç»\u001CîrºË1ì.Ç¸»ÜÎ\u0001¼Ë1ô.ÇØ»\u001Cïr¾Ë1ü.Çø»\u001C\u0003ðrÀË1\u0004/÷ó¹9Þå\u0018c\u0018^qx9\u0006âå\u0018c(^±x9\u0006ãå\u0018Çùø\u001Dïr\fÈË1\"/Ç¼\u001Ccòr\fÊË1*/Ç°¼\u001Cãòr\fÌËó|w9Ææå\u0018ct^áy9Æçå\u0018 c^!z9Æèåã\fw9éå\u0018§c ^z9êå\u0018«c°^Ñz9ëåóSw9Fìå\u0018²cÌ^A{9Fíå\u0018¶cÜ^{9FîåëÎ¹é±{=Æîõ\u0018»×cì^±{=Æîõ\u0018»×cì^±{½SÜñ.ÇØ½\u001Ec÷zÝë1v¯ÇØ½\u001Ec÷zÝë9O\u0013eÍÇ»så9Y³å9]óå9a\u001Ec÷zÝë1v¯·sÂ=Þå\u0018»×cì^±{=Æîõ\u0018»×cì^±{=Æîõ\u0018»×û9o\u001FïrÝë1v¯ÇØ½\u001Ec÷zÝë1v¯ÇØ½\u001Ec÷zÝëãþw9Æîõ\u0018»×cì^±{=Æîõ\u0018»×cì^±{=Æîõy®\"Ç»\u001Cc÷zÝë1v¯ÇØ½\u001Ec÷zÝë1v¯ÇØ½\u001Ec÷úq.FÇ»\u001Cc÷zÝë1v¯ÇØ½\u001E\n"
     ]
    }
   ],
   "source": [
    "file_enc = \"latin-1\"\n",
    "with smart_open.open(zip_fpath, 'rb', encoding=file_enc) as infile:\n",
    "    print(gensim.utils.to_unicode(infile.read(1000)))\n",
    "    #for line in infile:\n",
    "    #    print(line)\n",
    "    #    line = gensim.utils.to_unicode(line).split(\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x97 in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m###Set parameters. Details here: https://radimrehurek.com/gensim/models/word2vec.html###\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword2vec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWord2Vec\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43msg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnegative\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw2vmodel_ng5_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(year_1)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(year_2)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_full\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m syn0_object\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39msyn0\n",
      "File \u001B[1;32m~\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    428\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m(epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 429\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m    431\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count,\n\u001B[0;32m    432\u001B[0m         total_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs, start_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha,\n\u001B[0;32m    433\u001B[0m         end_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_alpha, compute_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\lib\\site-packages\\gensim\\models\\word2vec.py:491\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \n\u001B[0;32m    455\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    488\u001B[0m \n\u001B[0;32m    489\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 491\u001B[0m total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_vocab\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words \u001B[38;5;241m=\u001B[39m total_words\n",
      "File \u001B[1;32m~\\lib\\site-packages\\gensim\\models\\word2vec.py:586\u001B[0m, in \u001B[0;36mWord2Vec.scan_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file:\n\u001B[0;32m    584\u001B[0m     corpus_iterable \u001B[38;5;241m=\u001B[39m LineSentence(corpus_file)\n\u001B[1;32m--> 586\u001B[0m total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_scan_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcollected \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m word types from a corpus of \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m raw words and \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m sentences\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_vocab), total_words, corpus_count\n\u001B[0;32m    591\u001B[0m )\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_words, corpus_count\n",
      "File \u001B[1;32m~\\lib\\site-packages\\gensim\\models\\word2vec.py:555\u001B[0m, in \u001B[0;36mWord2Vec._scan_vocab\u001B[1;34m(self, sentences, progress_per, trim_rule)\u001B[0m\n\u001B[0;32m    553\u001B[0m vocab \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m    554\u001B[0m checked_string_types \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 555\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence_no, sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentences):\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m checked_string_types:\n\u001B[0;32m    557\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sentence, \u001B[38;5;28mstr\u001B[39m):\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mMySentsSimple.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# for each compressed file open it\u001B[39;00m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m smart_open\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfpath, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m infile:\n\u001B[1;32m---> 38\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mislice(infile, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlimit):\n\u001B[0;32m     39\u001B[0m                 line \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mto_unicode(line)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     40\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line)\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m3\u001B[39m:\n",
      "File \u001B[1;32mC:\\Python310\\lib\\codecs.py:322\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[0;32m    321\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[1;32m--> 322\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;66;03m# keep undecoded input until the next call\u001B[39;00m\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m=\u001B[39m data[consumed:]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0x97 in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "###Set parameters. Details here: https://radimrehurek.com/gensim/models/word2vec.html###\n",
    "model = gensim.models.word2vec.Word2Vec(sentences,sg=1, vector_size=300, window=5,\n",
    "                                        min_count=10, workers=10, hs=0, negative=8)\n",
    "model.save('w2vmodel_ng5_'+str(year_1)+'_'+str(year_2)+'_full')\n",
    "syn0_object=model.wv.syn0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "##output vector space##\n",
    "numpy.savetxt('syn0_ngf_'+str(year_1)+'_'+str(year_2)+'_full.txt',syn0_object,delimiter=\" \")\n",
    "\n",
    "#output vocab list#\n",
    "vocab_list = model.wv.index2word\n",
    "for i in range(0,len(vocab_list)):\n",
    "    if vocab_list[i] == '':\n",
    "        vocab_list[i] = \"thisisanemptytoken\"+str(i)\n",
    "\n",
    "with open('vocab_list_ngf_'+str(year_1)+'_'+str(year_2)+'_full.txt','wb') as outfile:\n",
    "    for i in range(0,len(vocab_list)):\n",
    "        outfile.write(vocab_list[i].encode('utf8')+\"\\n\".encode('ascii'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}